{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfPJhFCtP8o0"
      },
      "source": [
        "# EVSSM Project\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qu-fgjC3P8o2",
        "outputId": "08562838-cb77-44d4-cba9-136fc19b4a58"
      },
      "execution_count": 1,
      "source": [
        "import torch\n",
        "print('PyTorch version:', torch.__version__)\n",
        "print('CUDA available:', torch.cuda.is_available())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.9.0+cu126\n",
            "CUDA available: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y950HrOlP8o2",
        "outputId": "d6b30aea-94b3-4317-9fae-48a0386e01ae"
      },
      "execution_count": 2,
      "source": [
        "!rm -rf /content/EVSSM\n",
        "!git clone https://github.com/kkkls/EVSSM.git /content/EVSSM\n",
        "!pip install -q basicsr einops yacs opencv-python tqdm scikit-image timm matplotlib\n",
        "print('✓ Repo cloned and dependencies installed.')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into '/content/EVSSM'...\n",
            "remote: Enumerating objects: 306, done.\u001b[K\n",
            "remote: Counting objects: 100% (306/306), done.\u001b[K\n",
            "remote: Compressing objects: 100% (281/281), done.\u001b[K\n",
            "remote: Total 306 (delta 42), reused 249 (delta 16), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (306/306), 682.55 KiB | 2.47 MiB/s, done.\n",
            "Resolving deltas: 100% (42/42), done.\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.5/172.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.8/46.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.4/299.4 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m114.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.2/256.2 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for basicsr (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "✓ Repo cloned and dependencies installed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JigCpGL-P8o3",
        "outputId": "5362ebd1-d9bf-461e-d1a6-350c90a40140"
      },
      "execution_count": 3,
      "source": [
        "import sys, os\n",
        "sys.path.append('/content/EVSSM')\n",
        "print(os.listdir('/content/EVSSM'))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['test.py', 'VERSION', 'Real_blur_PSNR.py', '.git', 'options', 'environment.yml', 'train.sh', 'README.md', 'license', 'setup.cfg', 'setup.py', 'basicsr', 'test.sh', 'models', 'scripts', '.idea']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "SQJwLcutP8o3",
        "outputId": "eed0ee1c-e159-45ca-df65-8ff4c170e8bd"
      },
      "execution_count": 4,
      "source": [
        "!sed -n '1,200p' /content/EVSSM/models/EVSSM.py"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "import torch\n",
            "import torch.nn as nn\n",
            "import torch.nn.functional as F\n",
            "import numbers\n",
            "from einops import rearrange, repeat\n",
            "import math\n",
            "from mamba_ssm.ops.selective_scan_interface import selective_scan_fn, selective_scan_ref\n",
            "\n",
            "from torchvision.transforms.functional import resize, to_pil_image  # type: ignore\n",
            "import numpy as np\n",
            "\n",
            "\n",
            "def to_3d(x):\n",
            "    return rearrange(x, 'b c h w -> b (h w) c')\n",
            "\n",
            "\n",
            "def to_4d(x, h, w):\n",
            "    return rearrange(x, 'b (h w) c -> b c h w', h=h, w=w)\n",
            "\n",
            "\n",
            "\n",
            "class WithBias_LayerNorm(nn.Module):\n",
            "    def __init__(self, normalized_shape):\n",
            "        super(WithBias_LayerNorm, self).__init__()\n",
            "        if isinstance(normalized_shape, numbers.Integral):\n",
            "            normalized_shape = (normalized_shape,)\n",
            "        normalized_shape = torch.Size(normalized_shape)\n",
            "\n",
            "        assert len(normalized_shape) == 1\n",
            "\n",
            "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
            "        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n",
            "        self.normalized_shape = normalized_shape\n",
            "\n",
            "    def forward(self, x):\n",
            "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + 1e-6) * self.weight + self.bias\n",
            "\n",
            "\n",
            "class LayerNorm(nn.Module):\n",
            "    def __init__(self, dim):\n",
            "        super(LayerNorm, self).__init__()\n",
            "\n",
            "        self.body = WithBias_LayerNorm(dim)\n",
            "\n",
            "    def forward(self, x):\n",
            "        h, w = x.shape[-2:]\n",
            "        return to_4d(self.body(to_3d(x)), h, w)\n",
            "\n",
            "\n",
            "class EDFFN(nn.Module):\n",
            "    def __init__(self, dim, ffn_expansion_factor, bias):\n",
            "        super(EDFFN, self).__init__()\n",
            "\n",
            "        hidden_features = int(dim * ffn_expansion_factor)\n",
            "\n",
            "        self.patch_size = 8\n",
            "\n",
            "        self.dim = dim\n",
            "        self.project_in = nn.Conv2d(dim, hidden_features * 2, kernel_size=1, bias=bias)\n",
            "\n",
            "        self.dwconv = nn.Conv2d(hidden_features * 2, hidden_features * 2, kernel_size=3, stride=1, padding=1,\n",
            "                                groups=hidden_features * 2, bias=bias)\n",
            "\n",
            "        self.fft = nn.Parameter(torch.ones((dim, 1, 1, self.patch_size, self.patch_size // 2 + 1)))\n",
            "        self.project_out = nn.Conv2d(hidden_features, dim, kernel_size=1, bias=bias)\n",
            "\n",
            "    def forward(self, x):\n",
            "        x = self.project_in(x)\n",
            "        x1, x2 = self.dwconv(x).chunk(2, dim=1)\n",
            "        x = F.gelu(x1) * x2\n",
            "        x = self.project_out(x)\n",
            "\n",
            "        b, c, h, w = x.shape\n",
            "        h_n = (8 - h % 8) % 8\n",
            "        w_n = (8 - w % 8) % 8\n",
            "        \n",
            "        x = torch.nn.functional.pad(x, (0, w_n, 0, h_n), mode='reflect')\n",
            "        x_patch = rearrange(x, 'b c (h patch1) (w patch2) -> b c h w patch1 patch2', patch1=self.patch_size,\n",
            "                            patch2=self.patch_size)\n",
            "        x_patch_fft = torch.fft.rfft2(x_patch.float())\n",
            "        x_patch_fft = x_patch_fft * self.fft\n",
            "        x_patch = torch.fft.irfft2(x_patch_fft, s=(self.patch_size, self.patch_size))\n",
            "        x = rearrange(x_patch, 'b c h w patch1 patch2 -> b c (h patch1) (w patch2)', patch1=self.patch_size,\n",
            "                      patch2=self.patch_size)\n",
            "        \n",
            "        x=x[:,:,:h,:w]\n",
            "        \n",
            "        return x\n",
            "\n",
            "class SS2D(nn.Module):\n",
            "    def __init__(\n",
            "            self,\n",
            "            d_model,\n",
            "            d_state=8,\n",
            "            d_conv=3,\n",
            "            expand=2.,\n",
            "            dt_rank=\"auto\",\n",
            "            dt_min=0.001,\n",
            "            dt_max=0.1,\n",
            "            dt_init=\"random\",\n",
            "            dt_scale=1.0,\n",
            "            dt_init_floor=1e-4,\n",
            "            dropout=0.,\n",
            "            conv_bias=True,\n",
            "            bias=False,\n",
            "            device=None,\n",
            "            dtype=None,\n",
            "            **kwargs,\n",
            "    ):\n",
            "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
            "        super().__init__()\n",
            "        self.d_model = d_model\n",
            "        self.d_state = d_state\n",
            "        self.d_conv = d_conv\n",
            "        self.expand = expand\n",
            "        self.d_inner = int(self.expand * self.d_model)\n",
            "        self.dt_rank = math.ceil(self.d_model / 16) if dt_rank == \"auto\" else dt_rank\n",
            "\n",
            "        self.in_proj = nn.Linear(self.d_model, self.d_inner * 2, bias=bias, **factory_kwargs)\n",
            "        self.conv2d = nn.Conv2d(\n",
            "            in_channels=self.d_inner,\n",
            "            out_channels=self.d_inner,\n",
            "            groups=self.d_inner,\n",
            "            bias=conv_bias,\n",
            "            kernel_size=d_conv,\n",
            "            padding=(d_conv - 1) // 2,\n",
            "            **factory_kwargs,\n",
            "        )\n",
            "        self.act = nn.GELU()\n",
            "\n",
            "        self.x_proj = (\n",
            "            nn.Linear(self.d_inner, (self.dt_rank + self.d_state * 2), bias=False, **factory_kwargs),\n",
            "\n",
            "        )\n",
            "        self.x_proj_weight = nn.Parameter(torch.stack([t.weight for t in self.x_proj], dim=0))  # (K=4, N, inner)\n",
            "        del self.x_proj\n",
            "\n",
            "        self.x_conv = nn.Conv1d(in_channels=(self.dt_rank + self.d_state * 2), out_channels=(self.dt_rank + self.d_state * 2), kernel_size=7, padding=3,groups=(self.dt_rank + self.d_state * 2))\n",
            "\n",
            "        self.dt_projs = (\n",
            "            self.dt_init(self.dt_rank, self.d_inner, dt_scale, dt_init, dt_min, dt_max, dt_init_floor,\n",
            "                         **factory_kwargs),\n",
            "        )\n",
            "        self.dt_projs_weight = nn.Parameter(torch.stack([t.weight for t in self.dt_projs], dim=0))  # (K=4, inner, rank)\n",
            "        self.dt_projs_bias = nn.Parameter(torch.stack([t.bias for t in self.dt_projs], dim=0))  # (K=4, inner)\n",
            "        del self.dt_projs\n",
            "\n",
            "        self.A_logs = self.A_log_init(self.d_state, self.d_inner, copies=1, merge=True)  # (K=4, D, N)\n",
            "        self.Ds = self.D_init(self.d_inner, copies=1, merge=True)  # (K=4, D, N)\n",
            "\n",
            "        self.selective_scan = selective_scan_fn\n",
            "\n",
            "        self.out_norm = nn.LayerNorm(self.d_inner)\n",
            "        self.out_proj = nn.Linear(self.d_inner, self.d_model, bias=bias, **factory_kwargs)\n",
            "        self.dropout = nn.Dropout(dropout) if dropout > 0. else None\n",
            "\n",
            "    @staticmethod\n",
            "    def dt_init(dt_rank, d_inner, dt_scale=1.0, dt_init=\"random\", dt_min=0.001, dt_max=0.1, dt_init_floor=1e-4,\n",
            "                **factory_kwargs):\n",
            "        dt_proj = nn.Linear(dt_rank, d_inner, bias=True, **factory_kwargs)\n",
            "\n",
            "        # Initialize special dt projection to preserve variance at initialization\n",
            "        dt_init_std = dt_rank ** -0.5 * dt_scale\n",
            "        if dt_init == \"constant\":\n",
            "            nn.init.constant_(dt_proj.weight, dt_init_std)\n",
            "        elif dt_init == \"random\":\n",
            "            nn.init.uniform_(dt_proj.weight, -dt_init_std, dt_init_std)\n",
            "        else:\n",
            "            raise NotImplementedError\n",
            "\n",
            "        # Initialize dt bias so that F.softplus(dt_bias) is between dt_min and dt_max\n",
            "        dt = torch.exp(\n",
            "            torch.rand(d_inner, **factory_kwargs) * (math.log(dt_max) - math.log(dt_min))\n",
            "            + math.log(dt_min)\n",
            "        ).clamp(min=dt_init_floor)\n",
            "        # Inverse of softplus: https://github.com/pytorch/pytorch/issues/72759\n",
            "        inv_dt = dt + torch.log(-torch.expm1(-dt))\n",
            "        with torch.no_grad():\n",
            "            dt_proj.bias.copy_(inv_dt)\n",
            "        # Our initialization would set all Linear.bias to zero, need to mark this one as _no_reinit\n",
            "        dt_proj.bias._no_reinit = True\n",
            "\n",
            "        return dt_proj\n",
            "\n",
            "    @staticmethod\n",
            "    def A_log_init(d_state, d_inner, copies=1, device=None, merge=True):\n",
            "        # S4D real initialization\n",
            "        A = repeat(\n",
            "            torch.arange(1, d_state + 1, dtype=torch.float32, device=device),\n",
            "            \"n -> d n\",\n",
            "            d=d_inner,\n",
            "        ).contiguous()\n",
            "        A_log = torch.log(A)  # Keep A_log in fp32\n",
            "        if copies > 1:\n",
            "            A_log = repeat(A_log, \"d n -> r d n\", r=copies)\n",
            "            if merge:\n",
            "                A_log = A_log.flatten(0, 1)\n",
            "        A_log = nn.Parameter(A_log)\n",
            "        A_log._no_weight_decay = True\n",
            "        return A_log\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, os\n",
        "sys.path.append('/content/EVSSM')\n",
        "\n",
        "print(\"Repo contains:\", os.listdir('/content/EVSSM'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZciuT76OQhmH",
        "outputId": "af7754d0-cab4-4e15-9715-f939a9b727ba"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Repo contains: ['test.py', 'VERSION', 'Real_blur_PSNR.py', '.git', 'options', 'environment.yml', 'train.sh', 'README.md', 'license', 'setup.cfg', 'setup.py', 'basicsr', 'test.sh', 'models', 'scripts', '.idea']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **TASK 2 — Simplified EVSSM Model**"
      ],
      "metadata": {
        "id": "Ua2x9j5KQy5s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/evssm_nomamba.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange\n",
        "\n",
        "# -------- LayerNorm --------\n",
        "class WithBias_LN(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(dim))\n",
        "        self.bias = nn.Parameter(torch.zeros(dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        var = x.var(-1, keepdim=True, unbiased=False)\n",
        "        return (x - mean) / (var + 1e-5).sqrt() * self.weight + self.bias\n",
        "\n",
        "class LN2D(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.ln = WithBias_LN(dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,C,H,W = x.shape\n",
        "        t = rearrange(x, \"b c h w -> b (h w) c\")\n",
        "        t = self.ln(t)\n",
        "        return rearrange(t, \"b (h w) c -> b c h w\", h=H, w=W)\n",
        "\n",
        "# -------- EDFFN Block --------\n",
        "class EDFFN(nn.Module):\n",
        "    def __init__(self, dim, exp=3):\n",
        "        super().__init__()\n",
        "        hid = int(dim * exp)\n",
        "        self.inp = nn.Conv2d(dim, hid * 2, 1)\n",
        "        self.dw = nn.Conv2d(hid * 2, hid * 2, 3, padding=1, groups=hid * 2)\n",
        "        self.out = nn.Conv2d(hid, dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.inp(x)\n",
        "        x1, x2 = self.dw(x).chunk(2,1)\n",
        "        return self.out(F.gelu(x1) * x2)\n",
        "\n",
        "# -------- TinySS2D Block (Pure PyTorch SSM-like) --------\n",
        "class TinySS2D(nn.Module):\n",
        "    def __init__(self, dim, exp=2):\n",
        "        super().__init__()\n",
        "        hid = int(dim * exp)\n",
        "        self.inp = nn.Linear(dim, hid)\n",
        "        self.dw = nn.Conv2d(hid, hid, 3, padding=1, groups=hid)\n",
        "        self.ln = nn.LayerNorm(hid)\n",
        "        self.out = nn.Linear(hid, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,C,H,W = x.shape\n",
        "        t = rearrange(x, \"b c h w -> b h w c\")\n",
        "        t = self.inp(t)\n",
        "        t2 = rearrange(t, \"b h w c -> b c h w\")\n",
        "        t2 = F.gelu(self.dw(t2))\n",
        "        t2 = rearrange(t2, \"b c h w -> b h w c\")\n",
        "        t2 = self.ln(t2)\n",
        "        t = self.out(t2)\n",
        "        return rearrange(t, \"b h w c -> b c h w\")\n",
        "\n",
        "# -------- EVS Block --------\n",
        "class EVS(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.ln1 = LN2D(dim)\n",
        "        self.ssm = TinySS2D(dim)\n",
        "        self.ln2 = LN2D(dim)\n",
        "        self.ffn = EDFFN(dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.ssm(self.ln1(x))\n",
        "        x = x + self.ffn(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# -------- Full EVSSM No-Mamba Model --------\n",
        "class EVSSM_NoMamba(nn.Module):\n",
        "    def __init__(self, dim=48):\n",
        "        super().__init__()\n",
        "        self.enc = nn.Sequential(\n",
        "            nn.Conv2d(3, dim, 3, padding=1),\n",
        "            EVS(dim),\n",
        "            EVS(dim)\n",
        "        )\n",
        "        self.dec = nn.Sequential(\n",
        "            EVS(dim),\n",
        "            EVS(dim),\n",
        "            nn.Conv2d(dim, 3, 3, padding=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.enc(x)\n",
        "        y = self.dec(y)\n",
        "        return y + x  # residual connection\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CaRU6PBeQ7Cf",
        "outputId": "cf03656d-b7c2-4bb6-a8dd-f5f2653eebc7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/evssm_nomamba.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from evssm_nomamba import EVSSM_NoMamba\n",
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = EVSSM_NoMamba().to(device)\n",
        "\n",
        "x = torch.randn(1,3,256,256).to(device)\n",
        "y = model(x)\n",
        "\n",
        "print(\"Output:\", y.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2U9vfQfQRDOu",
        "outputId": "fec75704-c871-4729-9896-875f9e704f68"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: torch.Size([1, 3, 256, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Run Infrence**"
      ],
      "metadata": {
        "id": "luBpFOoxRwQN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/run_inference.py\n",
        "import os, torch\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from evssm_nomamba import EVSSM_NoMamba\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = EVSSM_NoMamba().to(device)\n",
        "model.eval()\n",
        "\n",
        "to_tensor = transforms.ToTensor()\n",
        "to_pil = transforms.ToPILImage()\n",
        "\n",
        "MAX_SIZE = 1024  # safe for Colab GPU\n",
        "\n",
        "def resize_if_needed(img):\n",
        "    w, h = img.size\n",
        "    if max(w, h) > MAX_SIZE:\n",
        "        scale = MAX_SIZE / max(w, h)\n",
        "        new_w = int(w * scale)\n",
        "        new_h = int(h * scale)\n",
        "        return img.resize((new_w, new_h), Image.LANCZOS)\n",
        "    return img\n",
        "\n",
        "input_dir = \"/content/blur_20\"\n",
        "output_dir = \"/content/outputs_20\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "image_files = sorted(os.listdir(input_dir))\n",
        "print(f\"Found {len(image_files)} files\\n\")\n",
        "\n",
        "for name in image_files:\n",
        "\n",
        "    if not name.lower().endswith((\".jpg\",\".jpeg\",\".png\",\".bmp\",\".tif\",\".tiff\")):\n",
        "        continue\n",
        "\n",
        "    print(\"Processing:\", name)\n",
        "    path = os.path.join(input_dir, name)\n",
        "\n",
        "    try:\n",
        "        img = Image.open(path).convert(\"RGB\")\n",
        "        img = resize_if_needed(img)\n",
        "    except Exception as e:\n",
        "        print(\"❌ ERROR loading\", name, \"→\", e)\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        x = to_tensor(img).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            pred = torch.clamp(model(x), 0, 1)\n",
        "\n",
        "        out_img = to_pil(pred.squeeze(0).cpu())\n",
        "        out_name = name.replace(\".png\",\"_out.png\")\n",
        "        out_path = os.path.join(output_dir, out_name)\n",
        "        out_img.save(out_path)\n",
        "\n",
        "        print(\"Saved:\", out_path, \"\\n\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"❌ ERROR processing\", name, \"→\", e)\n",
        "        continue\n",
        "\n",
        "print(\"✓ Finished all images!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JY_7PVYVHkx",
        "outputId": "06efdc91-807d-4d90-baea-4c44a70ff51e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/run_inference.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/run_inference.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aEY5-tTBR5h6",
        "outputId": "fb94bac4-c5ce-4a23-bff8-5181289da721"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/run_inference.py\", line 28, in <module>\n",
            "    image_files = sorted(os.listdir(input_dir))\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/content/blur_20'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U pip wheel setuptools\n",
        "!pip install ninja einops packaging\n",
        "\n",
        "# Install the correct Mamba SSM wheel (works in Colab)\n",
        "!pip install https://github.com/state-spaces/mamba/releases/download/v1.2.0/mamba_ssm-1.2.0-py3-none-manylinux2014_x86_64.whl\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 742
        },
        "id": "cib-_BejZYKc",
        "outputId": "36b7e6d1-afbc-427d-8f51-8e294bf7a4b9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-25.3-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.12/dist-packages (0.45.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (75.2.0)\n",
            "Collecting setuptools\n",
            "  Downloading setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Downloading pip-25.3-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m77.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: setuptools, pip\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 75.2.0\n",
            "    Uninstalling setuptools-75.2.0:\n",
            "      Successfully uninstalled setuptools-75.2.0\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pip-25.3 setuptools-80.9.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_distutils_hack"
                ]
              },
              "id": "554d40b435e14e86aca88c30fe01492d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ninja\n",
            "  Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (0.8.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (25.0)\n",
            "Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (180 kB)\n",
            "Installing collected packages: ninja\n",
            "Successfully installed ninja-1.13.0\n",
            "Collecting mamba-ssm==1.2.0\n",
            "\u001b[31m  ERROR: HTTP error 404 while getting https://github.com/state-spaces/mamba/releases/download/v1.2.0/mamba_ssm-1.2.0-py3-none-manylinux2014_x86_64.whl\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not install requirement mamba-ssm==1.2.0 from https://github.com/state-spaces/mamba/releases/download/v1.2.0/mamba_ssm-1.2.0-py3-none-manylinux2014_x86_64.whl because of HTTP error 404 Client Error: Not Found for url: https://github.com/state-spaces/mamba/releases/download/v1.2.0/mamba_ssm-1.2.0-py3-none-manylinux2014_x86_64.whl for URL https://github.com/state-spaces/mamba/releases/download/v1.2.0/mamba_ssm-1.2.0-py3-none-manylinux2014_x86_64.whl\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "cewcANWug3KT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27aa5312-577e-4b09-adb7-3fbd4a4b13b5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import sys\n",
        "\n",
        "# Install mamba-ssm if not already installed (previous attempt failed)\n",
        "!pip install mamba-ssm\n",
        "\n",
        "# Add EVSSM repo to path\n",
        "sys.path.append('/content/EVSSM')\n",
        "\n",
        "from models.EVSSM import EVSSM\n",
        "\n",
        "ckpt_path = \"/content/net_g_GoPro.pth\"\n",
        "\n",
        "# Load checkpoint\n",
        "ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
        "\n",
        "# Some checkpoints store params under ckpt[\"params\"], some under ckpt[\"state_dict\"]\n",
        "if \"params\" in ckpt:\n",
        "    weights = ckpt[\"params\"]\n",
        "elif \"state_dict\" in ckpt:\n",
        "    weights = ckpt[\"state_dict\"]\n",
        "else:\n",
        "    weights = ckpt  # fallback\n",
        "\n",
        "# Initialize model\n",
        "model = EVSSM()\n",
        "\n",
        "# Load the weights\n",
        "model.load_state_dict(weights, strict=True)\n",
        "\n",
        "# Move to GPU\n",
        "model = model.cuda().eval()\n",
        "\n",
        "print(\"✓ Loaded EVSSM model with net_g_GoPro.pth weights\")"
      ],
      "metadata": {
        "id": "OUvT1g34YsHd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 628
        },
        "outputId": "85f4b971-da91-4f23-e881-6aa711f5cc6f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'mamba_ssm'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1658694014.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/EVSSM'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEVSSM\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEVSSM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mckpt_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/net_g_GoPro.pth\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/EVSSM/models/EVSSM.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0meinops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrearrange\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmamba_ssm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselective_scan_interface\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mselective_scan_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselective_scan_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mresize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_pil_image\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mamba_ssm'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}